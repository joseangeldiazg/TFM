%?????????????????????????
% Nombre: capitulo7.tex  
% 
% Texto del capitulo 7
%---------------------------------------------------

\chapter{Análisis exploratorio de datos}
\label{Análisis exploratorio}

El análisis exploratorio de datos o EDA por sus siglas en inglés, se centra en gran medida en indagar en los datos que recibimos de la etapa de pre-procesado para comenzar a obtener información relevante o que pueda ser de utilizad para afinar en procesos posteriores o incluso anteriores. El análisis exploratorio, tiene su mayor aliado en los gráficos, ya que una representación gráfica de los mismos será de gran ayuda para comprender el problema y poder dirigir mejor las demás acciones. 

Este capítulo comienza por tanto en el estudio y visualización  y filtrado de los datos obtenidos en el capítulo \ref{limpieza-datos}, y finalizará con un análisis de sentimiento básico, necesario para posteriores etapas, donde trataremos de obtener los sentimientos asociados a las reglas de asociación que obtendremos en el capítulo \ref{minería}.

\section{Proceso exploratorio}

Llegados a este punto, sabemos que nuestro dataset está formado por 140.718 tuits que hacen referencia a alguna persona, no tienen caracteres raros o palabras vacías, contienen pocos enlaces y su contenido está en minúsculas. Pero, ¿qué hay del contenido de estos tuits? ¿cómo sabremos si nuestro posterior proceso de minería de datos obtendrá opiniones relevantes sobre personas? La respuesta a estas preguntas reside en los gráficos y su interpretación. 

Hasta el momento nos hemos dedicado a limpiar y procesar los datos usando técnicas aprobadas por la comunidad en cuanto a minería de textos respecta, pero es a partir de ahora cuando entra en juego el criterio y la técnica del analista de datos para comenzar a obtener información relevante y refinar los procesos de limpieza sobre los datos del problema en cuestión. 

\subsection{\textit{Term Document Matrix}}

En el ámbito de un problema de minería de textos, si queremos comenzar a trabajar y representar los datos, dependemos de una estructura de datos denominada \textit{\textbf{Term Document Matrix}}. La idea es poder representar el número de documentos donde un término aparece, tal y como podemos ver en el ejemplo de la tabla \ref{tdm}, donde tendríamos 5 documentos y  5 términos. 

\begin{table}[]
\centering
\begin{tabular}{|l|l|l|l|l|l|}
\hline
\textbf{TERM \textbackslash DOC} & \textbf{DOC1} & \textbf{DOC2} & \textbf{DOC3} & \textbf{DOC4} & \textbf{DOC5} \\ \hline
\textit{TERMINO1}                         & 0             & 1             & 0             & 0             & 1             \\ \hline
\textit{TERMINO2}                         & 0             & 0             & 0             & 1             & 0             \\ \hline
\textit{TERMINO3}                         & 0             & 1             & 0             & 0             & 0             \\ \hline
\textit{TERMINO4}                         & 1             & 1             & 0             & 0             & 1             \\ \hline
\textit{TERMINO5}                         & 0             & 1             & 1             & 0             & 0             \\ \hline
\end{tabular}
\caption{Ejemplo de matriz de frecuencias. }
\label{tdm}
\end{table}

Notar la facilidad de procesado de esta estructura para recuperar por ejemplo, el número de ocurrencias del término 1 (2 ocurrencias) y los documentos en los que aparece (doc2 y doc5). Cabe mencionar la posibilidad de encontrar como variante a esta estructura, otra denominada como \textit{\textbf{Document Term  Matrix}}, donde en la columna inicial tendríamos los documentos y en la fila los términos. 

En nuestro problema, tendremos tantos documentos como tuits tenemos, es decir 140.718, y tantas filas como palabras distintas puedan haberse usado en Twitter,  la variabilidad es tanta y la matriz es tan dispersa que sin procesado previo se hace imposible mantener una matriz tan grande en memoria por lo que es aquí donde el análisis exploratorio y la visualización toman un papel relevante.

\subsection{Reducción del problema basada en EDA}

El primer paso lógico para reducir el problema puede estar en acotar palabras raras o que tendrán poco valor en nuestro problema por ser poco comunes y por tanto nunca podrán ser parte de una tendencia u opinión recurrente. Consideraremos palabras raras aquellas cuya longitud por ejemplo exceda de 18 caracteres. Para ver qué palabras eran estas usamos un gráfico de nube de términos, y el resultado puede verse en la figura \ref{raras}. 

\begin{figure}[h]
\centering
\includegraphics[width=9cm]{./Capitulo7/imagenes/raras.png}
\caption{Nube de palabras raras.}
\label{raras}
\end{figure}

Parece que nuestro primer paso ha surtido efecto ya que hemos encontrado palabras y uniones de palabras provenientes de \textit{hashtags} que para nada son interesantes en nuestro problema, pero antes de borrarlas cabe la necesidad de estudiar cómo de frecuentes son estas palabras por lo que usaremos un histograma que contará el número de palabras en función de varios rangos de tamaños de las mismas. El resultado de este histograma podemos verlo en la figura \ref{freqraras}.

\begin{figure}[h]
\centering
\includegraphics[width=9cm]{./Capitulo7/imagenes/freqraras.png}
\caption{Frecuencia de palabras en función del tamaño.}
\label{freqraras}
\end{figure}

Acorde a este último gráfico, vemos como la variabilidad de nuestro problema se centra notablemente en palabras de tamaño [1-10] , lo que podríamos considerar palabras normales, por lo que acotaremos nuestro problema eliminando aquellas palabras un poco por encima de este margen, es decir, nos quedaremos con aquellas en el rango de tamaño [1-13] eliminando las de tamaño 14 y superiores donde encontramos entre otras las que vimos en el gráfico \ref{raras}.

Una vez acotadas estas palabras, nuestro problema se ha reducido a una matriz de 109790x140718, aún imposible de mantener en memoria, pero tal y como hemos mencionado antes, al estar en un problema donde el resultado final es buscar tendencias de opinión, un paso obvio sería cortar este dataset en función de la frecuencia de las palabras ya que una palabra que aparece en Twitter solo 5 o 6 veces nunca podrá ser considerada parte de una tendencia por lo que nuestra \textbf{\textit{term document matrix} } final estará solo compuesta por aquellas palabras que aparezcan más de 20 veces en el conjunto de tuits, reduciendo el dominio del problema a una matriz de 7323x140718, fácilmente manejable en memoria y donde por medio de suma y conteos en sus columnas y filas nos permitirán la representación de gráficos para una mejor comprensión del problema. 

\subsection{Visualización}

Al reducir el problema, podemos comenzar a realizar gráficos para obtener información relevante. Para ello, nos basaremos en dos de los gráficos más extendidos en minería de textos, las nubes de palabras y los gráficos de frecuencia o histogramas.

La nube de palabras parece una buena representación para hacernos una idea de qué se hablaba en Twitter durante aquellos meses por lo que comenzaremos con esta representación, acotando las palabras a las que aparecen por lo menos en 300 tuits. El resultado de este gráfico puede verse en la figura \ref{300}.

\begin{figure}[h]
\centering
\includegraphics[width=7cm]{./Capitulo7/imagenes/300.png}
\caption{Nube de palabras con 300 ocurrencias mínimo.}
\label{300}
\end{figure}

En este gráfico comenzamos a ver patrones interesantes, como por ejemplo los nombres propios de \textbf{Trump, Clinton, Obama, Drake, Simmons o Hillary}, entre otros, por lo que parece que el proceso NER descrito en la sección \ref{seleccion} ha funcionado correctamente. Por otro lado, la premisa de los objetivos del proyecto de obtener tendencias y opiniones sobre personas desde un enfoque no dirigido, está comenzando a cumplirse. 

Destacar la importancia en el proceso, de la aparición de estos nombres propios como frecuentes, pudiendo ya catalogarlos a los mismos de influencers, al menos, en este período de tiempo, que tras una investigación corresponde con la campaña electoral en EEUU. Debido a este evento político y social, a partir de ahora cabrá esperar temas políticos y opiniones sobre los mismos en nuestro proceso de minería de datos. 

Para finalizar este punto, se realizó un histograma para ver cuáles eran las palabras más frecuentes. El fin de este es obtener y entender posibles tendencias en Twitter en esta época, que como hemos averiguado corresponde a las elecciones presidenciales. El punto de corte para el gráfico lo situaremos en palabras con al menos 1700 ocurrencias, es decir, haremos un estudio de las tendencias más acentuadas en esta época. 


\begin{figure}[h]
\centering
\includegraphics[height=10cm]{./Capitulo7/imagenes/1700.png}
\caption{Palabras más frecuentes con al menos 1700 ocurrencias.}
\label{1700}
\end{figure}


Si estudiamos en detalle el gráfico \ref{1700} vemos como la tendencia en Twitter era hablar de \textbf{Trump o Clinton}, por lo que podremos centrar nuestros esfuerzos en el área política donde seguro obtendremos resultados e información relevante. Cabe también destacar, la presencia de nombres propios como, \textbf{donald}, que casi con probabilidad representará el nombre de pila del actual presidente de EEUU. Este factor es clave, ya que si minamos reglas de asociación, estas tendrán más fuerza, funcionarán mejor y eliminarán ruido, si pudiéramos relacionar o fusionar términos como \textbf{donald}, en un solo nombre propio del tipo \textbf{donald-trump} o \textbf{hillary-clinton}. Para  realizar esto, nos basaremos en un estudio basado en \textit{bi-gramas}. 

\subsection{N-gramas}

Los n-gramas, son una técnica extendida en minería de textos y recuperación de información, que se basa en la probabilidad de co-ocurrencia. Es decir, dato un término a estudiar los \textbf{n} términos siguientes al mismo para descubrir patrones como en nuestro caso, nombres propios formados por dos palabras. Realizaremos un estudio sobre nuestros tuits basado en \textit{2-gramas}, donde trataremos de comprobar si la premisa de que habrá una gran representación  de nombres compuestos que podremos unir para obtener mejores resultados en la etapa posterior de minería de datos. 

Para obtener  los bi-gramas, primero usamos un \textit{tokenizer} del paquete \textbf{RWeka} \cite{rweka}, tras lo cual, podremos usar normalmente la generación de gráficos de barras para ver cuáles son las palabras que aparecen juntas con más frecuencia. 

\begin{figure}[h]
\centering
\includegraphics[height=12cm]{./Capitulo7/imagenes/bigramas.png}
\caption{2-gramas más frecuentes.}
\label{bigramas}
\end{figure}

Acorde al gráfico \ref{bigramas}, podemos corroborar lo esperado de que los bi-gramas más comunes corresponderían a nombres propios, al menos, en gran medida. Debido a esto y para favorecer el siguiente proceso de obtención de reglas de asociación, fusionaremos los nombres propios más comunes de nuestro problema, para que sean tenidos en cuenta como una sola palabra en lugar de dos. 

Si paráramos en este punto, en base al conocimiento adquirido, podríamos obtener los temas tratados en Twitter durante aquel período de tiempo, así como categorizar sin ninguna duda cuáles eran las personas más influyentes e incluso qué políticas o sentimientos se podrían asociar con ellos. Queda constatada la potencia del filtrado y visualización de los datos así como la importancia de estas etapas en un proyecto de análisis de datos, pero, también cabe preguntarse si aún hay conocimiento de valor implícito en estos datos. El tratar de resolver esta pregunta será lo que abordaremos en el siguiente capítulo, donde desarrollaremos la última etapa de nuestro modelo, la etapa de \textbf{minería de datos}. 

\pagebreak
\clearpage
%---------------------------------------------------