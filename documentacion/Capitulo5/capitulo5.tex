%?????????????????????????
% Nombre: capitulo5.tex  
% 
% Texto del capitulo 5
%---------------------------------------------------

\chapter{Arquitectura del sistema}
\label{dataset}

En este capítulo se presenta la arquitectura del sistema diseñado, así como se detallan las tareas y funciones llevada a cabo por cada uno de los `módulos' que integran el sistema. Comenzaremos el capítulo indagando de manera gráfica en como se componen los módulos, la retroalimentación de los mismos y las tecnologías que usaremos en cada uno de ellos y finalizaremos detallando las tareas llevadas a cabo en cada uno de los módulos. 

\begin{figure}[H]
\centering
\includegraphics[width=11cm]{./Capitulo5/imagenes/arquitectura.png}
\caption{Arquitectura del modelo y módulos que lo integran. }
\label{arquitectura}
\end{figure}

En el gráfico \ref{arquitectura} se pueden ver un resumen de los `módulos'  que forman el sistema, que a grandes rasgos son las piezas fundamentales de cualquier proyecto de minería de datos. 

Descendiendo un nivel más de abstracción en el gráfico \ref{flujo}, encontramos definidas las tecnologías usadas en cada uno de los módulos vistos anteriormente y donde se detalla el flujo de información por el modelo creado. Dada la importancia del proceso de \textbf{minería de datos} en este proyecto, este módulo se verá mas detenidamente en el capítulo \ref{minería}.

\begin{figure}[h]
\centering
\includegraphics[width=12cm]{./Capitulo5/imagenes/flujo.png}
\caption{Flujo del proceso de obtención del modelo y tecnologías usadas. }
\label{flujo}
\end{figure}
 

\section{Obtención de datos}

En la primera etapa de nuestro modelo, lógicamente, se centra en la obtención de los datos para su posterior proceso. La obtención de los mismos, no es para nada trivial e implica conocimientos y herramientas de vertientes dispares de la ingeniería informática como pueden ser la programación de \textit{crawlers} \footnote{Programas informáticos que recorren la web en busca de información de diversa índole en función de para lo que estén programados.} o las bases de datos. 

\subsection{Carga de datos}
\label{obtenciondatos}

Como hemos visto en el punto \ref{api}, la obtención de los datos mediante la API de Twitter tiene serias restricciones a la hora de permitir peticiones de datos a la misma. Es por esto, que para la obtención de los datos, se usaron y probaron distintas herramientas y librerías disponibles de manera que esta tarea fuera lo más sencilla y eficiente posible. 

\subsubsection{Tweepy}

Tweepy \cite{tweepy} es una de las librerías de código abierto más extendidas entre la comunidad a la hora de conectar el lenguaje de programación Python con la API de Twitter. Esta librería ofrece distintos métodos y funciones útiles por ejemplo, para el proceso de conexión y autenticación de nuestras aplicaciones con la propia red social, así como también facilita la creación de métodos tanto para obtener datos en streaming (Streaming API) como por búsqueda (Search API).

Si nos centramos en la relevancia de esta librería respecto a nuestro proyecto, podríamos categorizarla como la primera herramienta que barajamos ya que se había visto a lo largo de los estudios de máster, pero rápidamente fue desechada ya que es imposible abolir las restricciones de peticiones a las API de Twitter, lo que hacía muy difícil, sino imposible, obtener una gran cantidad de datos  en un tiempo aceptable.

\subsubsection{Scrapy}

Scrapy \cite{scrapy}, al igual que en el caso anterior, es una librería \textit{open source} para Python que nos permite mediante una framework de desarrollo la creación de \textit{web crawlers}, conocidos como \textit{spiders}. Estos \textit{spiders}, sirven para recorrer la web, acorde a patrones previamente programados, y obtienen datos que pueden ser relevantes para múltiples funciones. 

Utilizando por tanto, esta herramienta y códigos de ejemplo disponibles en las especificaciones de la misma en internet, se modificó un crawler para recorrer la web de Twitter,  en un rango de fechas y lugar especificados. Estos datos, se obtenían de la página de búsqueda de Twitter por lo que permite evitar las restricciones de las API vistas anteriormente. Los pasos realizados por el crawler serían:

\begin{itemize}
	\item {Definición de parámetros, en nuestro caso fecha y lugar.}
	\item {El crawler comenzaría a buscar en la web de Twitter tuits acorde a nuestros parámetros.}
	\item {Dado que el html que ofrece esta web es muy fácil de parsear, se construye un objeto con los principales datos del tuit.}
	\item {Se almacena este objeto en la base de datos MongoDB con la que el programa ha conectado.}
\end{itemize}

Al finalizar el proceso de obtención de datos con la herramienta Scrapy, conseguimos un dataset formado por un total de 1.697.229 tuits, obtenidos en EEUU, entre los meses de enero y junio de 2016 y que son de habla inglesa. Los datos y especificaciones de los tuits recuperados podemos ver en la tabla \ref{tabla-datos}

\begin{table}[]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|l|l|}
\hline
\textbf{Variable} & \textbf{Tipo} & \textbf{Uso}                                     \\ \hline
ID                & Entero        & Identifica cada tuit en la red social.           \\ \hline
datetime          & String        & Contiene la fecha y la hora de emisión del tuit. \\ \hline
has\_media        & Booleano      & Indica si el tuit tiene elementos multimedia.    \\ \hline
is\_reply         & Booleano      & Indica si el tuit es una respuesta o no.         \\ \hline
is\_retweet       & Booleano      & Indica si el tuit es un RT  o no.                \\ \hline
nbr\_retweet      & Entero        & Indica el número de RTs que tiene el tuit.       \\ \hline
nbr\_favourite    & Entero        & Indica el número de favoritos que tiene el tuit. \\ \hline
nba\_reply        & Entero        & Indica el número de respuestas del tuit.         \\ \hline
text              & String        & Es el cuerpo del texto del tuit.                 \\ \hline
url               & String        & Urls que pueda haber en el tuit.                 \\ \hline
userid            & Entero        & Es el id del usuario emisor del tuit.            \\ \hline
usernameTweer     & Srting        & Es el nombre del usuario emisor del tuit         \\ \hline
\end{tabular}%
}
\caption{Especificaciones del dataset}
\label{tabla-datos}
\end{table}

Queda constatada la potencia del proceso de obtención de los datos ya que se ha generado un dataset muy rico y que se presta a la utilización del mismo en múltiples problemas que estudiaremos en el capítulo \ref{conclusiones} . Aún así, nuestro dataset se reducirá a un objeto de tipo \textit{\textbf{corpus}} donde nos quedaremos con el texto en cuestión, ya que es la parte más interesante para aplicar técnicas de minería de opiniones. 

\subsection{Persistencia de los datos}

Tras analizar los requisitos de los datos a almacenar y las operaciones que realizaríamos sobre ellos, la opción por la que el proyecto se ha decantado ha sido MongoDB \cite{mongo}. Esta base de datos es de tipo NoSQL y es la más extendida en procesos que van a trabajar con una gran cantidad de datos (Big Data). 

Dado que en nuestro problema no necesitamos una gran consistencia, sino una versatilidad y facilidad a la hora de trabajar con grandes volúmenes de datos, así como una gran facilidad para conectarse a las herramientas que veremos en puntos siguientes, nos hemos decantado por este sistema de base de datos. 

\subsection{Integración de datos}
\label{carga}
Una vez obtenidos y almacenados los datos en MongoDB, el siguiente paso lógico del problema es pasarlos a RStudio, donde por medio de nuestros scripts comenzaríamos con el tratamiento de los mismos. Es en este punto, donde topamos con el primer problema que nos lleva a un enfoque basado en Big Data dado que ninguna de las herramientas nativas de R ni las conexiones directas de R con MongoDB de paquetes como \textit{rmongodb} pueden manejar el dataset completo para obtener 1.7M  de documentos almacenados en MongoDB y pasar su contenido a un tipo de dato \textit{data-frame} de R. 

La solución, la encontramos en el paquete \textbf{\textit{SparkR}}. Este paquete \cite{sparkr}, crea una sesión distribuida por medio de virtualización (figura \ref{sparkrdis}) en Spark que ofrece funciones de filtrado, agregación y selección entre otras muchas, de manera similar a como se podría hacer con los dataframes nativos de R, con la diferencia de que al ser desde un enfoque distribuido, hace uso de unos objetos denominados, \textit{SparkDataFrames}. Estos objetos, pueden manejar grandes colecciones de datos ya que los distribuyen en columnas, que pueden ser construidas como en nuestro caso con una base de datos noSQL externa, aunque hay otras técnicas viables.

\begin{figure}[h]
\centering
\includegraphics[width=7cm]{./Capitulo5/imagenes/arq.png}
\caption{Arquitectura de Spark R.}
\label{sparkrdis}
\end{figure}

Una vez obtenidos los datos, en nuestra sesión de Spark en RStudio, debemos pasarlos a la sesión básica de R. Esto es así debido a que  Rstudio es el anfitrión de Spark, pero las sesiones difieren, por lo que debemos operar entre ambas por medio de funciones básicas de Big Data como \textit{collect} para el caso que nos compete de fusionar nuestro SparkDataFrame a un DataFrame de R. 

\section{Pre-procesado}

El pre-procesado de datos es una de las tareas más importantes en un proyecto de minería de datos. Podríamos definirlo como aquellas técnicas enmarcadas en ciencia de datos cuya finalidad es la de obtener datos de mayor calidad, más comprensibles, de menor dimensión y que puedan ser tratados apropiadamente por aquellas técnicas de minería de datos o \textit{machine learning} que habría que aplicar después.  La importancia de este proceso viene dada por motivos tales como:

\begin{enumerate}
\item Los datos en origen pueden ser impuros o de mala calidad, lo que conducirá a malos modelos una vez apliquemos minería de datos. 
\item El pre-procesado, puede generar un conjunto de datos de dimensiones inferiores al original, con la consiguiente mejora que esto ofrece. 
\item Al final del proceso de pre-procesado de datos, obtendremos datos de calidad, o al menos, mejores que si no fueran pre-procesados. Por consiguiente, los modelos basados en minería de datos, funcionarán mejor y podrán ser en muchos casos más interpretables. 
\end{enumerate}

En esta sección veremos las técnicas de tratamiento de datos utilizadas para la limpieza y refinamiento de un dataset que pueda ser usado para procesos posteriores como la visualización y la minería de datos. Dado el volumen y la naturaleza del problema, donde prácticamente cada uno de los 1.7M de tuits contiene algún elemento que hace que sea totalmente distinto de los demás, el pre-procesado de datos se hizo parte vital del modelo, siendo una de las etapas que más tiempo ha requerido del total del invertido en el proyecto. 

Los procesos de pre-procesado de datos abarcan métodos que van desde la preparación de los datos, hasta la reducción de variables u observaciones, pasando por distintos métodos de limpieza como filtrados de ruido, eliminación de palabras vacías o imputación de valores perdidos entre otros. Dado que nuestro problema, podría enmarcarse en minería de textos, las técnicas usadas vienen marcadas por distintas técnicas de tratamiento de textos y procesado del lenguaje natural que veremos a continuación.

\subsection{Preparación}

Al finalizar la etapa vista en la sección \ref{carga} tendremos datos en forma de DataFrame, o lo que es lo mismo, similares a una tabla. De cara a aplicar técnicas de minería de textos, estamos manteniendo mucha más información (tabla \ref{tabla-datos}) que la necesaria para nuestro análisis del campo \textit{text} de nuestros tuits.  Lo ideal sería una estructura de datos que cumpliera las siguientes premisas:

\begin{itemize}
	\item Pudiera trabajar de manera eficiente con grandes conjuntos de textos. 
	\item Mantuviera para cada texto (tuit) metadatos de manejos de cadenas, como longitud, ids.
\end{itemize}

La solución a estas necesidades reside por tanto en los objetos de tipo \textbf{\textit{Corpus}} del paquete \textbf{tm} \cite{tm} de minería de textos para R. Con simples instrucciones, tendremos integrados todos los tuits en nuestro \textit{corpus} de tal manera, en la que cada tuit es considerado un documento independiente en nuestra colección. 

\subsection{Limpieza}
\label{limpieza}

El proceso de limpieza, ha sido sencillo ya que son pasos bastante estandarizados en el campo de la minería de textos. En resumen, las técnicas aplicadas han sido:

\begin{enumerate}
	\item Eliminación de palabras vacías en inglés. A estas se le ha añadido la palabra \textbf{\textit{via}}, que podemos considerar vacía en el ámbito que nos incumbe. 
	\item Eliminación de enlaces. Dado el alcance del problema, esta tarea ha conllevado la localización de las principales redes sociales que se usan para compartir enlaces en Twittter, tales como Facebook, Youtube, SmartURL, Vine, OwLy o BitLy entre otras varias. El motivo de esta localización ha sido la elaboración de expresiones regulares con las que por medio de funciones se han eliminado sus ocurrencias.
	\item Eliminación de signos de puntuación y caracteres no alfanuméricos. 
\end{enumerate}


Cabe remarcar que hemos obviado el proceso de \textit{steaming} o lo que es lo mismo, guardar solo las raíces léxicas de cada palabra, debido a que consideramos que se podría perder interpretabilidad de cara a los procesos posteriores de obtención de reglas de asociación.

\subsection{Valores perdidos}

Tras el proceso de limpieza anterior en un volumen tan grande de datos cabe esperar que algún documento (tuit) estuviera formado por tan solo palabras vacías, enlaces o combinaciones de estos, es por ello, que por medio de filtrado básico de R se obtienen aquellos que no contienen ninguna palabra y se elimina del conjunto del dataset para evitar problemas en los procesos posteriores.

\subsection{Selección de instancias}
\label{seleccion}
La selección de instancias trata de obtener un conjunto de datos de dimensión inferior al original de manera que los procesos posteriores de minería puedan manejar mejor estos datos, u obtener información con valor de una manera menos influenciada por el ruido de observaciones no relevantes para el problema en cuestión. Dado el gran volumen de datos que manejamos, y la variedad infinita de temáticas posibles que se pudieran estar hablando en Twitter durante los meses de obtención de datos, cabe esperar en la etapa de minería de datos una gran  explosión de itemsets frecuentes y  reglas de asociación, que en la mayoría de los casos no serían relevantes para el estudio de tendencias u opiniones respecto a personas, lo cual es nuestro objetivo. Por ello, parece un paso claro que antes de llegar a etapas superiores del modelo, filtremos y eliminemos aquellos tuits que no hacen referencia a personas. Necesitamos por tanto reconocer las entidades presentes en un tuit y esto puede hacerse usando la técnica de \textit{Name Entity Recognition} \cite{ner} , de ahora en adelante NER por sus siglas en inglés.  Propuesto por la Universidad de Stanford, el método está implementado en Java, aunque viene integrado en diversos paquetes para R, por lo que permite de una manera sencilla la obtención de `\textit{entidades nombradas}' en un texto en función del tipo que deseemos buscar, como por ejemplo, personas, lugares, empresas e incluso monedas. El algoritmo usado para este proceso ha sido el siguiente:  
 
 
\begin{algorithm}[H]
\begin{algorithmic} 
	\ForAll {tuit in tuits}
		\State $string \gets entidad(tuit)$
			\If {String distinto de null}
            			\State $listaNombres \gets string$
        			\Else
             			\State $listaNombres \gets idTuit$
			\EndIf
	\EndFor			
 	\ForAll {elemento en listaNombres}
  		 \If {elemento.id = tuit.id}
     			borramos el tuit
   		\Else
   			 \State $datasetFinal \gets tuit$
		\EndIf
	\EndFor
\end{algorithmic}  
\caption{Obtiene los tuits que hacen referencia a personas}	
\end{algorithm}


Este proceso, es muy lento y se llevo a cabo en el cluster de computación visto en la tabla \ref{cluster}. Pese a la complejidad del proceso, sus características técnicas hacen que sea fácilmente paralelizable con las herramientas para programación paralela que R ofrece, por lo que se favorece y reduce bastante su tiempo de ejecución. En este punto se investigaron las opciones más rápidas de ejecución así como las mejores opciones para optimizar el código en R al máximo. Las configuraciones y pruebas realizadas, cuyos tiempos de ejecución pueden verse la tabla \ref{tabla-tiempos}, fueron las siguientes:

\begin{itemize}
\item Ejecución secuencial. 
\item Ejecución en paralelo con 3 núcleos. 
\item Utilización de la función lapply \footnote{Es una función implementada en C que permite de manera muy eficiente la aplicación de una determinada función a listas y vectores} en lugar de bucles.
\item Sustitución de los bloques \textit{if-else} por la función \textit{ifelse()} de R.
\item Sustitución de los bloques \textit{if-else} por la función \textit{ifelse()} de R + Ejecución en paralelo con 3 núcleos. 
\item Utilización de la función \textit{ifelse()} de R, junto con ejecución paralela en un cluster con 31 núcleos. 
\end{itemize}


Tras la ejecución del proceso \textit{NER}, obtenemos resultados bastante aceptables donde se localizan 140.718 tuits que hacen referencia a personas.  Sobre este conjunto de tuits se vuelven a aplicar un nuevo refinamiento de las técnicas de limpieza vistas en la sección \ref{limpieza}, a las que se añade el \textbf{paso a minúsculas} de todo el contenido. Este paso había sido obviado anteriormente para favorecer el proceso de \textit{NER}.

\begin{table}[]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|l|l|l|l|l|}
\hline
\textbf{METODO}                        & \textbf{10 TUITS} & \textbf{100 TUITS} & \textbf{1000 TUITS} & \textbf{10000 TUITS} & \textbf{1.7M TUITS}  \\ \hline
\textit{SECUENCIAL}                    & 47s 148ms         & 8min 23s 756ms     & 1h 21min 29s 29ms   & -                    & -                    \\ \hline
\textit{PARALELO (3 CORES)}                      & 7s 737ms          & 1min 15s 212ms     & 11min 21s 953ms     & 1h 47min 26s 858ms   & -                    \\ \hline
\textit{LAPPLY}                        & 47s 712ms         & 8min               & 15min 54s 123ms     & -                    & -                    \\ \hline
\textit{PARALELO (3 CORES) CON IFELSE}           & 18s 483ms         & 1min 5s 682ms      & 8min 49s 789ms      & 1h 29min 39s 516ms   & -                    \\ \hline
\textit{CLUSTER (PARALELO CON IFELSE)} & 1min 55s 682ms    & 1min 54s 453ms     & 2min 57s 810ms      & 20min 15s 324ms      & 1d 5h 0min 31s 836ms \\ \hline
\end{tabular}%
}
\caption{Tiempos de ejecución del proceso NER con distintos métodos}
\label{tabla-tiempos}
\end{table}

Como resumen final una vez finalizado el pre-procesado de datos, hemos conseguido reducir nuestro dataset de 1.7M de instancias a 140.718 limpias de caracteres raros o palabras vacías, donde además, todas y cada una de ellas hablan o hacen referencia a personas, por lo que muy seguramente con los procesos posteriores de análisis exploratorio de datos y minería de datos podremos obtener información relevante. 

\section{Filtrado y selección de datos}

Llegados a este punto, sabemos que nuestro dataset está formado por 140.718 tuits que hacen referencia a alguna persona, no tienen caracteres raros o palabras vacías, contienen pocos enlaces y su contenido está en minúsculas. Pero, ¿qué hay del contenido de estos tuits? ¿cómo sabremos si nuestro posterior proceso de minería de datos obtendrá opiniones relevantes sobre personas? La respuesta a estas preguntas reside en los gráficos y su interpretación. 

Hasta el momento nos hemos dedicado a limpiar y procesar los datos usando técnicas aprobadas por la comunidad en cuanto a minería de textos respecta, pero es a partir de ahora cuando entra en juego el criterio y la técnica del analista de datos para comenzar a obtener información relevante y refinar los procesos de limpieza sobre los datos del problema en cuestión. 

\subsection{\textit{Term Document Matrix}}

En el ámbito de un problema de minería de textos, si queremos comenzar a trabajar y representar los datos, dependemos de una estructura de datos denominada \textit{\textbf{Term Document Matrix}}. La idea es poder representar el número de documentos donde un término aparece, tal y como podemos ver en el ejemplo de la tabla \ref{tdm}, donde tendríamos 5 documentos y  5 términos. 

\begin{table}[]
\centering
\begin{tabular}{|l|l|l|l|l|l|}
\hline
\textbf{TERM \textbackslash DOC} & \textbf{DOC1} & \textbf{DOC2} & \textbf{DOC3} & \textbf{DOC4} & \textbf{DOC5} \\ \hline
\textit{TERMINO1}                         & 0             & 1             & 0             & 0             & 1             \\ \hline
\textit{TERMINO2}                         & 0             & 0             & 0             & 1             & 0             \\ \hline
\textit{TERMINO3}                         & 0             & 1             & 0             & 0             & 0             \\ \hline
\textit{TERMINO4}                         & 1             & 1             & 0             & 0             & 1             \\ \hline
\textit{TERMINO5}                         & 0             & 1             & 1             & 0             & 0             \\ \hline
\end{tabular}
\caption{Ejemplo de matriz de frecuencias. }
\label{tdm}
\end{table}

Notar la facilidad de procesado de esta estructura para recuperar por ejemplo, el número de ocurrencias del término 1 (2 ocurrencias) y los documentos en los que aparece (doc2 y doc5). Cabe mencionar la posibilidad de encontrar como variante a esta estructura, otra denominada como \textit{\textbf{Document Term  Matrix}}, donde en la columna inicial tendríamos los documentos y en la fila los términos. 

En nuestro problema, tendremos tantos documentos como tuits tenemos, es decir 140.718, y tantas filas como palabras distintas puedan haberse usado en Twitter,  la variabilidad es tanta y la matriz es tan dispersa que sin procesado previo se hace imposible mantener una matriz tan grande en memoria por lo que es aquí donde el análisis exploratorio y la visualización toman un papel relevante.

\subsection{Filtrado basado en visualización}

El primer paso lógico para reducir el problema puede estar en acotar palabras raras o que tendrán poco valor en nuestro problema por ser poco comunes y por tanto nunca podrán ser parte de una tendencia u opinión recurrente. Consideraremos palabras raras aquellas cuya longitud por ejemplo exceda de 18 caracteres. 

Para hacernos una idea de manera rápida de cuales son estas palabras y sus frecuencias podremos utilizar una representacion en forma de \textbf{nube de términos}. Esta representación, fue usada por primera vez sobre la novela \textit{Microserfs}, donde por medio de un software creado por  Jim Flanagan se obtenían sus palabras y se representaban en función de colores y tamaños según su aparición en la novela teniendo en el centro de la nube aquellos términos más comunes o relevantes. Desde el auge de la web, los blogs y las redes sociales esta representación se ha hecho muy popular ya que permite en un solo vistazo el resumen de las temáticas de una web o blog por tomar algún ejemplo. La idea esencial de las nubes de palabras, reside por tanto en contabilizar las palabras o términos de un determinado documento o conjunto de documentos, eliminar aquellas que no dicen nada y representar en orden de más relevancia desde el centro hacia los bordes las palabras que también tendrán atributos como el color (más acentuado) o el tamaño (más grande) en función de su importancia o presencia. Usaremos por tanto un gráfico de nube de términos para obtener estas palabras raras y sus frecuencias en función del tamaño y color. El resultado puede verse en la figura \ref{raras}. 

Parece que nuestro primer paso ha surtido efecto ya que hemos encontrado palabras y uniones de palabras provenientes de \textit{hashtags} que para nada son interesantes en nuestro problema, pero antes de borrarlas cabe la necesidad de estudiar cómo de frecuentes son estas palabras por lo que usaremos un histograma que contará el número de palabras en función de varios rangos de tamaños de las mismas. El resultado de este histograma podemos verlo en la figura \ref{freqraras}.

\begin{figure}[h]
\centering
\includegraphics[width=9cm]{./Capitulo5/imagenes/raras.png}
\caption{Nube de palabras raras.}
\label{raras}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=12cm]{./Capitulo5/imagenes/freqraras.png}
\caption{Frecuencia de palabras en función del tamaño.}
\label{freqraras}
\end{figure}

Acorde a este último gráfico, vemos como la variabilidad de nuestro problema se centra notablemente en palabras de tamaño [1-10] , lo que podríamos considerar palabras normales, por lo que acotaremos nuestro problema eliminando aquellas palabras un poco por encima de este margen, es decir, nos quedaremos con aquellas en el rango de tamaño [1-13] eliminando las de tamaño 14 y superiores donde encontramos entre otras las que vimos en el gráfico \ref{raras}. Antes de realizar esta eliminación, es menester comprobar que lo que estamos eliminando beneficiará el modelo real y no producirá perdida de contenido por lo que se creará una nueva matriz de frecuencias con las palabras raras. Tras su creación, tendremos 25359 palabras que se quedan fuera del problema , los beneficios de eliminarlas son obvios, pues además a ser palabras largas ocuparan un gran espacio en memoria que dificultará los procesos anteriores. Por otro lado, estas palabras aumentan en un 25\% el dominio del problema con el consiguiente aumento de tiempos. Para demostrar que las palabras eliminadas no aportarían información extra al problema, se han realizado gráficos para comprobar cuales son, pero dada la longitud de estas ningún gráfico puede manejarlas bien, por lo que para favorecer esta interpretación en la figura \ref{salida} puede verse la salida de R, para algunas de estas palabras con su frecuencia.  

\begin{figure}[h]
\centering
\includegraphics[width=10cm]{./Capitulo5/imagenes/salida.png}
\caption{Algunas palabras fuera del dominio.}
\label{salida}
\end{figure}


Una vez acotadas estas palabras, nuestro problema se ha reducido a una matriz de 109790x140718, aún imposible de mantener en memoria, pero tal y como hemos mencionado antes, al estar en un problema donde el resultado final es buscar tendencias de opinión, un paso obvio sería cortar este dataset en función de la frecuencia de las palabras ya que una palabra que aparece en Twitter solo 5 o 6 veces nunca podrá ser considerada parte de una tendencia por lo que nuestra \textbf{\textit{term document matrix} } final estará solo compuesta por aquellas palabras que aparezcan más de 20 veces en el conjunto de tuits, reduciendo el dominio del problema a una matriz de 7323x140718, fácilmente manejable en memoria y donde por medio de suma y conteos en sus columnas y filas nos permitirán la representación de gráficos para una mejor comprensión del problema. 

\subsection{Selección basada en visualización}

Al reducir el problema, podemos comenzar a realizar gráficos para obtener información relevante. Para ello, nos basaremos en dos de los gráficos más extendidos en minería de textos, las nubes de palabras y los gráficos de frecuencia o histogramas.

La nube de palabras parece una buena representación para hacernos una idea de qué se hablaba en Twitter durante aquellos meses por lo que comenzaremos con esta representación, acotando las palabras a las que aparecen por lo menos en 300 tuits. El resultado de este gráfico puede verse en la figura \ref{300}.

\begin{figure}[h]
\centering
\includegraphics[width=8cm]{./Capitulo5/imagenes/300.png}
\caption{Nube de palabras con 300 ocurrencias mínimo.}
\label{300}
\end{figure}

En este gráfico comenzamos a ver patrones interesantes, como por ejemplo los nombres propios de \textbf{Trump, Clinton, Obama, Drake, Simmons o Hillary}, entre otros, por lo que parece que el proceso NER descrito en la sección \ref{seleccion} ha funcionado correctamente. Por otro lado, la premisa de los objetivos del proyecto de obtener tendencias y opiniones sobre personas desde un enfoque no dirigido, está comenzando a cumplirse. 

Destacar la importancia en el proceso, de la aparición de estos nombres propios como frecuentes, pudiendo ya catalogarlos a los mismos de influencers, al menos, en este período de tiempo, que tras una investigación corresponde con la campaña electoral en EEUU. Debido a este evento político y social, a partir de ahora cabrá esperar temas políticos y opiniones sobre los mismos en nuestro proceso de minería de datos. 

Para finalizar este punto, se realizó un histograma para ver cuáles eran las palabras más frecuentes. El fin de este es obtener y entender posibles tendencias en Twitter en esta época, que como hemos averiguado corresponde a las elecciones presidenciales. El punto de corte para el gráfico lo situaremos en palabras con al menos 1700 ocurrencias, es decir, haremos un estudio de las tendencias más acentuadas en esta época. 


\begin{figure}[h]
\centering
\includegraphics[height=10cm]{./Capitulo5/imagenes/1700.png}
\caption{Palabras más frecuentes con al menos 1700 ocurrencias.}
\label{1700}
\end{figure}


Si estudiamos en detalle el gráfico \ref{1700} vemos como la tendencia en Twitter era hablar de \textbf{Trump o Clinton}, por lo que podremos centrar nuestros esfuerzos en el área política donde seguro obtendremos resultados e información relevante. Cabe también destacar, la presencia de nombres propios como, \textbf{donald}, que casi con probabilidad representará el nombre de pila del actual presidente de EEUU. Este factor es clave, ya que si minamos reglas de asociación, estas tendrán más fuerza, funcionarán mejor y eliminarán ruido, si pudiéramos relacionar o fusionar términos como \textbf{donald}, en un solo nombre propio del tipo \textbf{donald-trump} o \textbf{hillary-clinton}. Para  realizar esto, nos basaremos en un estudio basado en \textit{bi-gramas}. 

\subsection{N-gramas}

Los n-gramas, son una técnica extendida en minería de textos y recuperación de información, que se basa en la probabilidad de co-ocurrencia. Es decir, dato un término a estudiar los \textbf{n} términos siguientes al mismo para descubrir patrones como en nuestro caso, nombres propios formados por dos palabras. Realizaremos un estudio sobre nuestros tuits basado en \textit{2-gramas}, donde trataremos de comprobar si la premisa de que habrá una gran representación  de nombres compuestos que podremos unir para obtener mejores resultados en la etapa posterior de minería de datos. 

Para obtener  los bi-gramas, primero usamos un \textit{tokenizer} del paquete \textbf{RWeka} \cite{rweka}, tras lo cual, podremos usar normalmente la generación de gráficos de barras para ver cuáles son las palabras que aparecen juntas con más frecuencia. 

\begin{figure}[h]
\centering
\includegraphics[height=12cm]{./Capitulo5/imagenes/bigramas.png}
\caption{2-gramas más frecuentes.}
\label{bigramas}
\end{figure}

Acorde al gráfico \ref{bigramas}, podemos corroborar lo esperado de que los bi-gramas más comunes corresponderían a nombres propios, al menos, en gran medida. Debido a esto y para favorecer el siguiente proceso de obtención de reglas de asociación, fusionaremos los nombres propios más comunes de nuestro problema, para que sean tenidos en cuenta como una sola palabra en lugar de dos. 

Si paráramos en este punto, en base al conocimiento adquirido, podríamos obtener los temas tratados en Twitter durante aquel período de tiempo, así como categorizar sin ninguna duda cuáles eran las personas más influyentes e incluso qué políticas o sentimientos se podrían asociar con ellos. Queda constatada la potencia del filtrado y visualización de los datos así como la importancia de estas etapas en un proyecto de análisis de datos, pero, también cabe preguntarse si aún hay conocimiento de valor implícito en estos datos. El tratar de resolver esta pregunta será lo que abordaremos en el siguiente capítulo, donde desarrollaremos la última etapa de nuestro modelo, la etapa de \textbf{minería de datos}. 



\pagebreak
\clearpage
%---------------------------------------------------