%?????????????????????????
% Nombre: capitulo5.tex  
% 
% Texto del capitulo 5
%---------------------------------------------------

\chapter{Dataset}
\label{dataset}

En este capítulo se detalla el conjunto de datos empleado para la elaboración del sistema de minería de opiniones basado en reglas que veremos en capítulos siguientes. Como estos datos provienen de la red social Twitter, se lleva a cabo una pequeña introducción a la misma para continuar con una explicación del proceso llevado a cabo para la obtención del \textit{data set}.

\section{Twitter}

Twitter nace en Estados Unidos en el año 2006, partiendo de la idea de los antiguos mensajes de texto (SMS) limitó el número de caracteres en cada tuit a 140 favoreciendo que el intercambio de información fuera rápido, conciso y fluido, dando comienzo a una nueva vertiente en la web 2.0 que posteriormente se conocería como \textit{microblogging}.

El crecimiento de la red social en los últimos años ha sido exponencial, hecho que no la ha alejado de tener serios problemas de rentabilidad, pero que confirman su éxito y aceptación por parte del gran público. A principios de 2010 el número de usuarios activos al mes de la misma se fijaba en torno a los 30 millones, número claramente superado en la actualidad donde se estima en torno a los 313 millones de usuarios activos mensuales (Dreamgrow Marketing, 2017).

Recientemente, la red social ha aumentado el número de palabras en cada tuit a 280, lo que en conjunción con nuevas medidas como la facilidad para incluir videos o demás contenido multimedia y lo vistoso de estas publicaciones con un diseño intachable, constatan la salud de la red social. 

\subsection{Funcionamiento}

El funcionamiento de la red social en sí es muy sencillo, en esta podemos acotar un rol muy sencillo, el de \textbf{seguidor} que serán aquellas personas que quieren seguir nuestras publicaciones y de las cuales podremos ser seguidores o no, es decir, puede no es de obligada existencia el carácter bidireccional en una relación de `amistad' dentro de esta red social al igual que existe en otras redes sociales como por ejemplo Facebook. 

Este tipos de relaciones, son muy interesantes y han sido estudiados en la literatura como parte de la teoría de grafos y extendidos a la minería de redes sociales, para la detección de comunidades o de personas influyentes dentro de la red social \cite{grafos}. Dado que nuestro trabajo versa sobre la minería de opiniones, no es menester entrar en más detalle entre las relaciones entre usuarios dentro de twitter (\textit{retweets}, \textit{follows} ). Por otro lado, si que es necesario dado nuestro problema, diseccionar las partes que componen un tuit para ver su estrecha relación con los trabajo de minería de opiniones y la importancia de estos datos en este ámbito de la minería de datos. 

\subsection{Anatomía de un tuit}
\label{anatomia}

En la figura \ref{tuit}, podemos ver el ejemplo de un tuit real. Este puede tener variantes como enlaces o imágenes, pero esencialmente es texto y algunos \textit{hashtags} o etiquetas que sirven para acotar u opinar sobre temas en concreto.  

\begin{figure}[h]
\centering
\includegraphics[width=15cm]{./Capitulo5/imagenes/tuit.png}
\caption{Ejemplo de un tuit.}
\label{tuit}
\end{figure}

Atendiendo al ejemplo anterior (figura \ref{tuit}), estaríamos hablando sobre \textit{data science}, R, Pythton y el portal Data Camp. Cabe destacar, que aunque en este caso todos van precedidos del carácter \# esto no tendría porque ser así, y alguna de estas palabras podría aparecer sin éste. 

Tracemos ahora por tanto un pequeño paralelismo entre el tuit del ejemplo anterior y la definición dada por Liu de qué es una opinión que hemos podido ver en en la sección \ref{mineriaopiniones}. 

\begin{enumerate}
\item \textbf{Entidad}: En el caso de un tuit, la entidad sería sobre lo que se opina. En el caso del ejemplo anterior, sería \#DataCamp. 
\item \textbf{Emisor}: El paralelismo es obvio, el emisor es en el caso de un tuit la persona que lo tuitea en este caso, el usuario \@joseangeldiazg. 
\item \textbf{Aspecto}: Recoge lo que se valora, y aunque puede parecer abstracto del anterior tuit podemos deducir que se valora la capacidad de un portal en internet para formar a las personas sobre conceptos tales como \textit{data science}, R o Python. 
\item \textbf{Orientación}: En el caso que nos ocupa, es positiva. 
\item \textbf{Momento temporal}: Este elemento de la quintupla definida por Liu, al igual que el emisor siempre está presente dentro de un tuit y en este caso corresponde con el 26 de julio de 2017. 
\end{enumerate}

Es por tanto evidente, la relación entre un tuit y una opinión, poniendo al descubierto la importancia de este tipo de datos en el proceso y estudio de la minería de opiniones. Por otro lado, también es menester remarcar que no todos los tuits podrían enmarcarse dentro de la definición de opinión, como por ejemplo el que podemos ver en al figura \ref{tuit2} que simplemente es informativo. 

\begin{figure}
\includegraphics[width=10cm]{./Capitulo5/imagenes/tuit2.png}
\centering
\caption{Ejemplo de un tuit que no sería enmarcado como opinión.}
\label{tuit2}
\end{figure}

\subsection{Twitter API}
\label{api}
Twitter abrió sus datos al mundo al hacer disponible una serie de APIs mediante las cuales se permite a terceros tanto la obtención de estos datos para su estudio como la implementación de software que trabaje sobre estos datos. Estas APIs necesitan del protocolo de seguridad y autenticación OAuth, además ofrecen ciertas limitaciones a la hora de obtener los datos por lo que solo se permiten entre 150 y 300 solicitudes por hora y además hay una ventana temporal cerrará el flujo de información cada 15 minutos. Las APIs disponibles son:

\begin{itemize}
\item \textbf{Search API}: Obtiene los tuits de hasta 7 días, es similar a lo que nos ofrecería la búsqueda básica de Twitter en la interfaz web al buscar por un termino. 
\item \textbf{Streaming API}: Obtiene información en tiempo real. 
\item \textbf{REST API}: Obtiene los datos mediante HTTP, el formato puede venir en XML, HTML, o JSON, la limitación aquí viene definida por el número de resultados devueltos por página que no puede ser superior a 3200 tweets. 
\end{itemize}

Como podemos observar, estas limitaciones pueden suponer un gran problema a la hora de obtener una gran cantidad de datos para que nuestro trabajo tenga un cierto rigor y peso, por ello, en los puntos siguiente ahondaremos en el proceso seguido y tecnologías usadas para la obtención y almacenamiento de un gran volumen de datos a pesar de estas restricciones. 

\section{Persistencia de los datos}

Tras analizar los requisitos de los datos a almacenar y las operaciones que realizaríamos sobre ellos, la opción por la que el proyecto se ha decantado ha sido MongoDB \cite{mongo}. Esta base de datos es de tipo NoSQL y es la más extendida en procesos que van a trabajar con una gran cantidad de datos (Big Data). 

Dado que en nuestro problema no necesitamos una gran consistencia, sino una versatilidad y facilidad a la hora de trabajar con grandes volúmenes de datos, así como una gran facilidad para conectarse a las herramientas que veremos en el punto anterior, nos hemos decantado por este sistema de base de datos. 

\section{Obtención de datos}
\label{obtenciondatos}

Como hemos visto en el punto \ref{api}, la obtención de los datos mediante la API de Twitter tiene serias restricciones a la hora de permitir peticiones de datos a la misma. Es por esto, que para la obtención de los datos, se usaron y probaron distintas herramientas y librerías disponibles de manera que esta tarea fuera lo más sencilla y eficiente posible. 

\subsection{Tweepy}

Tweepy \cite{tweepy} es una de las librerías de código abierto más extendidas entre la comunidad a la hora de conectar el lenguaje de programación Python con la API de Twitter. Esta librería ofrece distintos métodos y funciones útiles por ejemplo, para el proceso de conexión y autenticación de nuestras aplicaciones con la propia red social, así como también facilita la creación de métodos tanto para obtener datos en streaming (Streaming API) como por búsqueda (Search API).

Si nos centramos en la relevancia de esta librería respecto a nuestro proyecto, podríamos categorizarla como la primera herramienta que barajamos ya que se había visto a lo largo de los estudios de máster, pero rápidamente fue desechada ya que es imposible abolir las restricciones de peticiones a las API de Twitter, lo que hacia muy difícil, sino imposible, obtener una gran cantidad de datos  en un tiempo aceptable.

\subsection{Scrapy}

Scrapy \cite{scrapy}, al igual que en el caso anterior, es una librería \textit{open source} para Python que nos permite mediante una framework de desarrollo la creación de \textit{web crawlers}, conocidos como \textit{spiders}. Estos \textit{spiders}, sirven para recorrer la web, acorde a patrones previamente programados, y obtienen datos que pueden ser relevantes para múltiples funciones. 

Utilizando por tanto, esta herramienta y códigos de ejemplo disponibles en las especificaciones de la misma en internet, se modificó un crawler para recorrer la web de Twitter,  en un rango de fechas y lugar especificados. Estos datos, se obtenían de la página de búsqueda de Twitter por lo que permite evitar las restricciones de las API vistas anteriormente. Los pasos realizados por el crawler serían:

\begin{itemize}
	\item {Definición de parámetros, en nuestro caso fecha y lugar.}
	\item {El crawler comenzaría a buscar en la web de twitter tuits acorde a nuestros parámetros.}
	\item {Dado que el html que ofrece esta web es muy fácil de parsear, se construye un objeto con los principales datos del tuit.}
	\item {Se almacena este objeto en la base de datos MongoDB con la que el programa ha conectado.}
\end{itemize}


\section{Especificaciones del dataset}

Una vez vista la naturaleza de los datos, el método de almacenamiento y  el proceso de obtención de los mismos, es turno de hablar de las especificaciones técnicas del mismo. 

El dataset está formado por un total de 1.697.229 tuits, obtenidos en EEUU, entre los meses de enero y junio de 2016 y que son de habla inglesa. Estos tuits se organizan en un dataframe de R cuyos datos y especificaciones podemos ver en la tabla \ref{tabla-datos}

\begin{table}[]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|l|l|}
\hline
\textbf{Variable} & \textbf{Tipo} & \textbf{Uso}                                     \\ \hline
ID                & Entero        & Identifica cada tuit en la red social.           \\ \hline
datetime          & String        & Contiene la fecha y la hora de emisión del tuit. \\ \hline
has\_media        & Booleano      & Indica si el tuit tiene elementos multimedia.    \\ \hline
is\_reply         & Booleano      & Indica si el tuit es una respuesta o no.         \\ \hline
is\_retweet       & Booleano      & Indica si el tuit es un RT  o no.                \\ \hline
nbr\_retweet      & Entero        & Indica el número de RTs que tiene el tuit.       \\ \hline
nbr\_favourite    & Entero        & Indica el número de favoritos que tiene el tuit. \\ \hline
nba\_reply        & Entero        & Indica el número de respuestas del tuit.         \\ \hline
text              & String        & Es el cuerpo del texto del tuit.                 \\ \hline
url               & String        & Urls que pueda haber en el tuit.                 \\ \hline
userid            & Entero        & Es el id del usuario emisor del tuit.            \\ \hline
usernameTweer     & Srting        & Es el nombre del usuario emisor del tuit         \\ \hline
\end{tabular}%
}
\caption{Especificaciones del dataset}
\label{tabla-datos}
\end{table}

Queda constatada la potencia del proceso de obtención de los datos ya que se ha generado un dataset muy rico y que se presta a la utilización del mismo en múltiples problemas que estudiaremos en el capítulo \ref{conclusiones} . Aún así, nuestro dataset se reducirá a un objeto de tipo \textit{\textbf{corpus}} donde nos quedaremos con el texto en cuestión, ya que es la parte más interesante para aplicar técnicas de minería de opiniones. 

\pagebreak
\clearpage
%---------------------------------------------------