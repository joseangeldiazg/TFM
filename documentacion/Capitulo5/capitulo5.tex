%?????????????????????????
% Nombre: capitulo5.tex  
% 
% Texto del capitulo 5
%---------------------------------------------------

\chapter{Dataset}
\label{dataset}

En este capítulo se detalla el conjunto de datos empleado para la elaboración del sistema de minería de opiniones basado en reglas que veremos en capítulos siguientes. Como estos datos provienen de la red social Twitter, se lleva a cabo una pequeña introducción a la misma para continuar con una explicación del proceso llevado a cabo para la obtención del \textit{dataset}.

\section{Persistencia de los datos}

Tras analizar los requisitos de los datos a almacenar y las operaciones que realizaríamos sobre ellos, la opción por la que el proyecto se ha decantado ha sido MongoDB \cite{mongo}. Esta base de datos es de tipo NoSQL y es la más extendida en procesos que van a trabajar con una gran cantidad de datos (Big Data). 

Dado que en nuestro problema no necesitamos una gran consistencia, sino una versatilidad y facilidad a la hora de trabajar con grandes volúmenes de datos, así como una gran facilidad para conectarse a las herramientas que veremos en el punto anterior, nos hemos decantado por este sistema de base de datos. 

\section{Obtención de los datos}
\label{obtenciondatos}

Como hemos visto en el punto \ref{api}, la obtención de los datos mediante la API de Twitter tiene serias restricciones a la hora de permitir peticiones de datos a la misma. Es por esto, que para la obtención de los datos, se usaron y probaron distintas herramientas y librerías disponibles de manera que esta tarea fuera lo más sencilla y eficiente posible. 

\subsection{Tweepy}

Tweepy \cite{tweepy} es una de las librerías de código abierto más extendidas entre la comunidad a la hora de conectar el lenguaje de programación Python con la API de Twitter. Esta librería ofrece distintos métodos y funciones útiles por ejemplo, para el proceso de conexión y autenticación de nuestras aplicaciones con la propia red social, así como también facilita la creación de métodos tanto para obtener datos en streaming (Streaming API) como por búsqueda (Search API).

Si nos centramos en la relevancia de esta librería respecto a nuestro proyecto, podríamos categorizarla como la primera herramienta que barajamos ya que se había visto a lo largo de los estudios de máster, pero rápidamente fue desechada ya que es imposible abolir las restricciones de peticiones a las API de Twitter, lo que hacía muy difícil, sino imposible, obtener una gran cantidad de datos  en un tiempo aceptable.

\subsection{Scrapy}

Scrapy \cite{scrapy}, al igual que en el caso anterior, es una librería \textit{open source} para Python que nos permite mediante una framework de desarrollo la creación de \textit{web crawlers}, conocidos como \textit{spiders}. Estos \textit{spiders}, sirven para recorrer la web, acorde a patrones previamente programados, y obtienen datos que pueden ser relevantes para múltiples funciones. 

Utilizando por tanto, esta herramienta y códigos de ejemplo disponibles en las especificaciones de la misma en internet, se modificó un crawler para recorrer la web de Twitter,  en un rango de fechas y lugar especificados. Estos datos, se obtenían de la página de búsqueda de Twitter por lo que permite evitar las restricciones de las API vistas anteriormente. Los pasos realizados por el crawler serían:

\begin{itemize}
	\item {Definición de parámetros, en nuestro caso fecha y lugar.}
	\item {El crawler comenzaría a buscar en la web de Twitter tuits acorde a nuestros parámetros.}
	\item {Dado que el html que ofrece esta web es muy fácil de parsear, se construye un objeto con los principales datos del tuit.}
	\item {Se almacena este objeto en la base de datos MongoDB con la que el programa ha conectado.}
\end{itemize}


\section{Especificaciones del dataset}

Una vez vista la naturaleza de los datos, el método de almacenamiento y  el proceso de obtención de los mismos, es turno de hablar de las especificaciones técnicas del mismo. 

El dataset está formado por un total de 1.697.229 tuits, obtenidos en EEUU, entre los meses de enero y junio de 2016 y que son de habla inglesa. Estos tuits se organizan en un dataframe de R cuyos datos y especificaciones podemos ver en la tabla \ref{tabla-datos}

\begin{table}[]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|l|l|}
\hline
\textbf{Variable} & \textbf{Tipo} & \textbf{Uso}                                     \\ \hline
ID                & Entero        & Identifica cada tuit en la red social.           \\ \hline
datetime          & String        & Contiene la fecha y la hora de emisión del tuit. \\ \hline
has\_media        & Booleano      & Indica si el tuit tiene elementos multimedia.    \\ \hline
is\_reply         & Booleano      & Indica si el tuit es una respuesta o no.         \\ \hline
is\_retweet       & Booleano      & Indica si el tuit es un RT  o no.                \\ \hline
nbr\_retweet      & Entero        & Indica el número de RTs que tiene el tuit.       \\ \hline
nbr\_favourite    & Entero        & Indica el número de favoritos que tiene el tuit. \\ \hline
nba\_reply        & Entero        & Indica el número de respuestas del tuit.         \\ \hline
text              & String        & Es el cuerpo del texto del tuit.                 \\ \hline
url               & String        & Urls que pueda haber en el tuit.                 \\ \hline
userid            & Entero        & Es el id del usuario emisor del tuit.            \\ \hline
usernameTweer     & Srting        & Es el nombre del usuario emisor del tuit         \\ \hline
\end{tabular}%
}
\caption{Especificaciones del dataset}
\label{tabla-datos}
\end{table}

Queda constatada la potencia del proceso de obtención de los datos ya que se ha generado un dataset muy rico y que se presta a la utilización del mismo en múltiples problemas que estudiaremos en el capítulo \ref{conclusiones} . Aún así, nuestro dataset se reducirá a un objeto de tipo \textit{\textbf{corpus}} donde nos quedaremos con el texto en cuestión, ya que es la parte más interesante para aplicar técnicas de minería de opiniones. 

\pagebreak
\clearpage
%---------------------------------------------------